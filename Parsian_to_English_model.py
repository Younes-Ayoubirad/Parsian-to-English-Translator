# ==============================================================================
#
# Persian-to-English Translation Fine-tuning Script
#
# Description:
# This script provides a comprehensive pipeline for fine-tuning a
# sequence-to-sequence model (MT5-base) for Persian-to-English translation.
# It leverages Hugging Face Transformers, Datasets, and Evaluate libraries.
#
# Key Features:
# - Configuration class for managing hyperparameters.
# - Custom cache directory setup for Hugging Face assets.
# - Data loading and shuffling for representative subset selection.
# - Robust preprocessing tailored for the specified dataset.
# - Fine-tuning with Seq2SeqTrainer, including:
#   - Mixed precision training (fp16) if GPU is available.
#   - Gradient accumulation for larger effective batch sizes.
#   - Gradient clipping for training stability.
#   - Evaluation using BLEU score.
#   - Early stopping to prevent overfitting.
# - Professional logging throughout the pipeline.
# - Inference with the fine-tuned model on sample sentences.
#
# Prerequisites:
# Ensure you have the necessary libraries installed, e.g.:
# pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
# pip install transformers==4.41.2 # Or a compatible stable version
# pip install datasets sentencepiece sacremoses accelerate evaluate sacrebleu
#
# ==============================================================================

import os
import sys
import logging
import torch
import numpy as np
from datasets import load_dataset, DatasetDict
from transformers import (
    MT5Tokenizer,
    AutoModelForSeq2SeqLM,
    DataCollatorForSeq2Seq,
    Seq2SeqTrainingArguments,
    Seq2SeqTrainer,
    EarlyStoppingCallback,
    pipeline
)
import evaluate  # For BLEU score calculation

# --- Step 0: Configure Logging and Cache Directory ---
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
    stream=sys.stdout,
)
logger = logging.getLogger(__name__)

try:
    # Set a custom directory for Hugging Face cache (models, datasets)
    # This line must be executed before any Hugging Face library imports that might trigger caching.
    os.environ['HF_HOME'] = 'H:/AI/Models/huggingface_cache'
    logger.info(f"Hugging Face cache directory set to: {os.environ['HF_HOME']}")
except Exception as e:
    logger.warning(f"Could not set HF_HOME. Using default Hugging Face cache location. Error: {e}")


# --- Configuration Class ---
class TrainingConfig:
    """Stores all configuration parameters for the training pipeline."""
    MODEL_CHECKPOINT: str = "persiannlp/mt5-base-parsinlu-opus-translation_fa_en"
    DATASET_NAME: str = "persiannlp/parsinlu_translation_fa_en"
    OUTPUT_DIR: str = "./persian_translator_finetuned_model"  # More descriptive output directory

    # Data handling
    USE_SUBSET_FOR_DEMO: bool = True
    TRAIN_SUBSET_SIZE: int = 1000
    EVAL_SUBSET_SIZE: int = 200
    SHUFFLE_SEED: int = 42

    # Preprocessing
    MAX_INPUT_LENGTH: int = 128
    MAX_TARGET_LENGTH: int = 128
    SOURCE_LANG_COL: str = "source"  # Column name for Persian text in the dataset
    TARGET_LANG_COL: str = "targets"  # Column name for English text in the dataset

    # Training hyperparameters
    NUM_TRAIN_EPOCHS: int = 3
    TRAIN_BATCH_SIZE: int = 4  # Per-device training batch size
    EVAL_BATCH_SIZE: int = 8  # Per-device evaluation batch size
    LEARNING_RATE: float = 2e-5  # Initial learning rate for AdamW optimizer
    GRADIENT_ACCUMULATION_STEPS: int = 4  # Number of steps to accumulate gradients before updating weights
    MAX_GRAD_NORM: float = 1.0  # Max gradient norm for clipping
    WEIGHT_DECAY: float = 0.01

    # Early stopping
    EARLY_STOPPING_PATIENCE: int = 3  # Stop training if metric doesn't improve for this many evaluations


# Global tokenizer variable, initialized after loading
tokenizer: MT5Tokenizer = None


def compute_bleu_metrics(eval_preds):
    """
    Computes BLEU score and generated length for translation evaluation.
    This function is passed to the Trainer's compute_metrics argument.
    It handles decoding of predictions and labels, and robustly calculates BLEU.
    """
    predictions_ids, label_ids = eval_preds
    if isinstance(predictions_ids, tuple):  # Predictions might be a tuple
        predictions_ids = predictions_ids[0]

    if tokenizer is None:
        logger.error("Tokenizer is not globally initialized for compute_bleu_metrics.")
        return {"bleu": 0.0, "gen_len": 0.0}

    # Ensure predictions are integer arrays for decoding
    predictions_ids = np.array(predictions_ids, dtype=np.int64)

    # Replace potentially invalid token IDs generated by the model
    # (e.g., negative values or values outside vocab size) with pad_token_id.
    vocab_size = tokenizer.vocab_size
    pad_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else 0  # Default to 0 if no pad_token

    invalid_preds_mask = (predictions_ids < 0) | (predictions_ids >= vocab_size)
    if np.any(invalid_preds_mask):
        num_invalid = np.sum(invalid_preds_mask)
        logger.warning(
            f"Found {num_invalid} out-of-vocabulary token IDs in model predictions. "
            f"Examples of invalid IDs: {predictions_ids[invalid_preds_mask][:5]}. "
            f"These will be replaced with pad_token_id ({pad_id}) before decoding."
        )
        predictions_ids[invalid_preds_mask] = pad_id

    decoded_preds = tokenizer.batch_decode(predictions_ids, skip_special_tokens=True)

    # Prepare labels: replace -100 (ignore_index) with pad_token_id for decoding
    label_ids = np.where(label_ids != -100, label_ids, pad_id)
    decoded_labels = tokenizer.batch_decode(label_ids, skip_special_tokens=True)

    # Post-process: strip whitespace and wrap labels in lists for sacrebleu
    decoded_preds = [pred.strip() for pred in decoded_preds]
    decoded_labels = [[label.strip()] for label in decoded_labels]

    try:
        sacrebleu_metric = evaluate.load("sacrebleu")
        result = sacrebleu_metric.compute(predictions=decoded_preds, references=decoded_labels)
        metrics = {"bleu": result["score"]}
    except Exception as e:
        logger.error(f"Error computing BLEU score: {e}")
        metrics = {"bleu": 0.0}  # Fallback BLEU score

    # Calculate average length of generated sequences
    try:
        prediction_lengths = [np.count_nonzero(pred_seq != pad_id) for pred_seq in predictions_ids]
        metrics["gen_len"] = np.mean(prediction_lengths)
    except Exception as e:
        logger.error(f"Error computing generated length: {e}")
        metrics["gen_len"] = 0.0

    return {k: round(v, 4) for k, v in metrics.items()}


def run_translation_pipeline(config: TrainingConfig):
    """
    Executes the full translation model fine-tuning pipeline:
    1. Loads and prepares the dataset.
    2. Loads the tokenizer and pre-trained model.
    3. Preprocesses (tokenizes) the data.
    4. Fine-tunes the model using Seq2SeqTrainer.
    5. Performs inference on sample sentences with the fine-tuned model.
    """
    global tokenizer  # Allow this function to set the global tokenizer

    logger.info("Starting translation fine-tuning pipeline...")
    os.makedirs(config.OUTPUT_DIR, exist_ok=True)
    logger.info(f"All outputs will be saved to: {config.OUTPUT_DIR}")

    # --- Step 1: Load and Prepare Dataset ---
    logger.info(f"Loading dataset: '{config.DATASET_NAME}'...")
    try:
        raw_datasets = load_dataset(config.DATASET_NAME)
    except Exception as e:
        logger.error(f"Failed to load dataset '{config.DATASET_NAME}'. Error: {e}")
        sys.exit(1)

    # Validate dataset structure
    required_columns = {config.SOURCE_LANG_COL, config.TARGET_LANG_COL}
    if "train" not in raw_datasets or not required_columns.issubset(raw_datasets["train"].column_names):
        logger.error(
            f"Training split in dataset '{config.DATASET_NAME}' is missing or does not contain "
            f"required columns: {config.SOURCE_LANG_COL}, {config.TARGET_LANG_COL}. "
            f"Available columns: {raw_datasets.get('train', {}).column_names}"
        )
        sys.exit(1)

    # Use 'test' split as 'validation' if 'validation' is not present
    if "validation" not in raw_datasets and "test" in raw_datasets:
        raw_datasets["validation"] = raw_datasets["test"]
        logger.info("Using 'test' split as 'validation' split as no 'validation' split was found.")

    if config.USE_SUBSET_FOR_DEMO:
        logger.info(
            f"Using a shuffled subset for demonstration: "
            f"{config.TRAIN_SUBSET_SIZE} train samples, {config.EVAL_SUBSET_SIZE} eval samples (seed={config.SHUFFLE_SEED})."
        )
        if 'train' in raw_datasets and len(raw_datasets['train']) >= config.TRAIN_SUBSET_SIZE:
            raw_datasets["train"] = raw_datasets["train"].shuffle(seed=config.SHUFFLE_SEED).select(
                range(config.TRAIN_SUBSET_SIZE))
        elif 'train' in raw_datasets:
            logger.warning(
                f"Train set has only {len(raw_datasets['train'])} samples, less than requested {config.TRAIN_SUBSET_SIZE}. Using all available.")

        if 'validation' in raw_datasets and len(raw_datasets['validation']) >= config.EVAL_SUBSET_SIZE:
            raw_datasets["validation"] = raw_datasets["validation"].shuffle(seed=config.SHUFFLE_SEED).select(
                range(config.EVAL_SUBSET_SIZE))
        elif 'validation' in raw_datasets:
            logger.warning(
                f"Validation set has only {len(raw_datasets['validation'])} samples, less than requested {config.EVAL_SUBSET_SIZE}. Using all available.")
        elif 'train' in raw_datasets:  # Create a small validation set from train if none exists
            logger.warning("No validation set available after subsetting. Creating a small one from the training set.")
            if len(raw_datasets['train']) > config.EVAL_SUBSET_SIZE * 2:  # Ensure enough samples
                split = raw_datasets['train'].train_test_split(test_size=config.EVAL_SUBSET_SIZE,
                                                               seed=config.SHUFFLE_SEED)
                raw_datasets['train'] = split['train']
                raw_datasets['validation'] = split['test']
            else:
                logger.warning(
                    "Not enough training samples to create a meaningful validation split. Proceeding without it for demo.")

    logger.info(f"Final dataset structure for training:\n{raw_datasets}")

    # --- Step 2: Load Tokenizer and Model ---
    logger.info(f"Loading tokenizer and model from checkpoint: '{config.MODEL_CHECKPOINT}'...")
    try:
        tokenizer = MT5Tokenizer.from_pretrained(config.MODEL_CHECKPOINT)
        model = AutoModelForSeq2SeqLM.from_pretrained(config.MODEL_CHECKPOINT)

        # Ensure tokenizer has a pad token, crucial for MT5 and data collation
        if tokenizer.pad_token is None:
            logger.warning("Tokenizer does not have a pad_token. Adding '<pad>' as pad_token.")
            tokenizer.add_special_tokens({'pad_token': '<pad>'})
            model.resize_token_embeddings(len(tokenizer))  # Adjust model embeddings for new token
            # It's good practice to also set model.config.pad_token_id
            model.config.pad_token_id = tokenizer.pad_token_id

    except Exception as e:
        logger.error(f"Failed to load model or tokenizer from '{config.MODEL_CHECKPOINT}'. Error: {e}")
        sys.exit(1)

    logger.info("Tokenizer and model loaded successfully.")

    # --- Step 3: Preprocess (Tokenize) Data ---
    def preprocess_function(examples):
        """Tokenizes source and target texts for the model."""
        inputs = [str(ex) for ex in examples[config.SOURCE_LANG_COL]]

        raw_targets = examples[config.TARGET_LANG_COL]
        # Handle cases where target might be a single string or a list
        targets = [str(ex[0]) if isinstance(ex, list) and ex else str(ex) if ex is not None else "" for ex in
                   raw_targets]

        model_inputs = tokenizer(inputs, max_length=config.MAX_INPUT_LENGTH, truncation=True)

        # Tokenize targets using text_target for label preparation
        labels = tokenizer(text_target=targets, max_length=config.MAX_TARGET_LENGTH, truncation=True)
        model_inputs["labels"] = labels["input_ids"]
        return model_inputs

    logger.info("Tokenizing all dataset splits...")
    try:
        tokenized_datasets = raw_datasets.map(
            preprocess_function,
            batched=True,
            remove_columns=raw_datasets["train"].column_names if "train" in raw_datasets else None
        )
        logger.info("Data tokenization complete.")
    except Exception as e:
        logger.error(f"Error during data tokenization: {e}", exc_info=True)
        sys.exit(1)

    # --- Step 4: Fine-tune the Model ---
    logger.info("Setting up training arguments and trainer...")

    # Data collator handles dynamic padding
    data_collator = DataCollatorForSeq2Seq(
        tokenizer,
        model=model,
        padding=True,
        label_pad_token_id=-100  # Use -100 to ignore padding in loss calculation for labels
    )

    has_validation_set = "validation" in tokenized_datasets and tokenized_datasets.get("validation") and len(
        tokenized_datasets["validation"]) > 0

    training_args = Seq2SeqTrainingArguments(
        output_dir=config.OUTPUT_DIR,
        # Use 'eval_strategy' (newer name) if available, fallback to 'evaluation_strategy' for older transformers versions
        # For transformers==4.41.2, 'evaluation_strategy' is the correct name.
        evaluation_strategy="epoch" if has_validation_set else "no",
        save_strategy="epoch",
        learning_rate=config.LEARNING_RATE,
        per_device_train_batch_size=config.TRAIN_BATCH_SIZE,
        per_device_eval_batch_size=config.EVAL_BATCH_SIZE,
        gradient_accumulation_steps=config.GRADIENT_ACCUMULATION_STEPS,
        max_grad_norm=config.MAX_GRAD_NORM,
        weight_decay=config.WEIGHT_DECAY,
        save_total_limit=3,  # Keep only the last 3 checkpoints
        num_train_epochs=config.NUM_TRAIN_EPOCHS,
        predict_with_generate=True,  # Enable generation for metrics like BLEU during evaluation
        fp16=torch.cuda.is_available(),  # Enable mixed-precision training if a GPU is available
        logging_dir=f"{config.OUTPUT_DIR}/logs",
        logging_strategy="steps",
        logging_steps=50,  # Log training progress every 50 steps
        load_best_model_at_end=has_validation_set,
        metric_for_best_model="bleu" if has_validation_set else "loss",
        greater_is_better=True if has_validation_set else False,  # BLEU: higher is better; Loss: lower is better
        report_to=[],  # Disable all default reporting integrations (like TensorBoard, W&B) to avoid potential issues
        generation_max_length=config.MAX_TARGET_LENGTH,
        # generation_num_beams=4, # Optional: Uncomment to use beam search during evaluation
    )

    trainer = Seq2SeqTrainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_datasets.get("train"),  # Use .get() for safety
        eval_dataset=tokenized_datasets.get("validation") if has_validation_set else None,
        tokenizer=tokenizer,
        data_collator=data_collator,
        compute_metrics=compute_bleu_metrics if has_validation_set else None,
        callbacks=[EarlyStoppingCallback(
            early_stopping_patience=config.EARLY_STOPPING_PATIENCE)] if has_validation_set else [],
    )

    logger.info(
        f"Starting model fine-tuning... Effective batch size: "
        f"{config.TRAIN_BATCH_SIZE * config.GRADIENT_ACCUMULATION_STEPS * (torch.cuda.device_count() if torch.cuda.is_available() else 1)}"
    )
    logger.warning("Please ensure you have sufficient disk space in the output and cache directories.")

    try:
        if torch.cuda.is_available():
            logger.info("Clearing CUDA cache before training to free up VRAM...")
            torch.cuda.empty_cache()

        train_result = trainer.train()
        logger.info("Training successfully completed.")

        # Save the final model (best model if load_best_model_at_end=True) and tokenizer
        trainer.save_model()
        logger.info(f"Fine-tuned model saved to {config.OUTPUT_DIR}")

        # Log and save training metrics
        metrics = train_result.metrics
        trainer.log_metrics("train", metrics)
        trainer.save_metrics("train", metrics)
        trainer.save_state()  # Save trainer state for potential resuming

        # Perform a final explicit evaluation on the validation set if it exists
        if has_validation_set:
            logger.info("Performing final evaluation on the validation set...")
            eval_results = trainer.evaluate(metric_key_prefix="final_eval")
            trainer.log_metrics("final_eval", eval_results)
            trainer.save_metrics("final_eval", eval_results)
            logger.info(f"Final evaluation metrics: {eval_results}")

    except Exception as e:
        logger.error(f"An error occurred during model training or final evaluation: {e}", exc_info=True)
        sys.exit(1)

    # --- Step 5: Inference with the Fine-tuned Model ---
    logger.info("--- Testing the fine-tuned model with sample sentences ---")

    # Check if the model was saved correctly
    if not os.path.exists(os.path.join(config.OUTPUT_DIR, "pytorch_model.bin")) and \
            not os.path.exists(os.path.join(config.OUTPUT_DIR, "model.safetensors")):
        logger.error(f"Fine-tuned model not found in {config.OUTPUT_DIR}. Skipping inference.")
    else:
        try:
            # Load the fine-tuned model and tokenizer for inference
            translator_pipeline = pipeline(
                "text2text-generation",
                model=config.OUTPUT_DIR,
                tokenizer=config.OUTPUT_DIR,  # Ensure the fine-tuned tokenizer is loaded
                device=0 if torch.cuda.is_available() else -1
            )
            logger.info("Fine-tuned translation pipeline loaded successfully for inference.")

            persian_sentences_to_translate = [
                "سلام، حال شما چطور است؟",
                "این یک پروژه ترجمه ماشینی با استفاده از ترنسفورمرها است.",
                "یادگیری عمیق بسیار جذاب و کاربردی است.",
                "دانشگاه تهران بهترین دانشگاه ایران است و در قلب پایتخت قرار دارد."
            ]

            logger.info("Translating sample sentences:")
            # Using the pipeline directly on a list can be efficient
            translated_outputs = translator_pipeline(persian_sentences_to_translate,
                                                     max_length=config.MAX_TARGET_LENGTH)

            for original_sentence, translated_output in zip(persian_sentences_to_translate, translated_outputs):
                translated_text = translated_output['generated_text']
                logger.info(f"Persian: {original_sentence}\nEnglish (Fine-tuned): {translated_text}\n")

        except Exception as e:
            logger.error(f"Error during inference with the fine-tuned model: {e}", exc_info=True)

    logger.info("--- Translation pipeline finished ---")


if __name__ == "__main__":
    try:
        # Initialize configuration
        current_config = TrainingConfig()

        # Check for GPU availability
        if torch.cuda.is_available():
            logger.info(f"GPU available: {torch.cuda.get_device_name(0)} (CUDA Version: {torch.version.cuda})")
            logger.info(f"PyTorch version: {torch.__version__}")
        else:
            logger.warning("No GPU detected. Training will run on CPU, which will be significantly slower.")

        # Run the main translation pipeline
        run_translation_pipeline(current_config)

    except SystemExit:  # Allow sys.exit() to terminate the script cleanly
        pass
    except Exception as e:
        logger.critical(f"A critical error occurred in the main execution block: {e}", exc_info=True)
        sys.exit(1)
